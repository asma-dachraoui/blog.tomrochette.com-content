---
title: Intelligence
created: 2016-02-17
taxonomy:
  category: [Artificial General Intelligence]
  status: in progress
---

## Context

## Learned in this study

## Things to explore
* At what point does intelligence emerges from non-intelligence?
    * (GEB p411) Turing suggests that above a certain level of complexity a qualitative difference appears, so that "super-critical" machines will be quite unlike the simple ones hitherto envisaged
	* I believe intelligence emerges from complexity. By stacking simple concepts onto one another, intelligence appears.
* To understand intelligence as a phenomenon must include an understanding of learning (J.R. Quinlan Induction of decision trees)
* How can AGI/intelligence emerge out of randomness? Can it emerge out of randomness (or is that too improbable/operationally impossible)?
* What is intelligence? Is it measurable?

# Overview

# Recognizing intelligence
## Relative to your own
### Below
* Recognition of errors made in the past

### In the same range
* Problem resolution in which there is tradeoff between the solutions (no real winner)

### Over
* Recognition of more effective solutions to similarly faced problems

# Measuring intelligence
* Factors of interest in the formula
	* Memory
	* Speed

# Quotes

> Do we want to copy the structure of human intelligence (i.e. the way our brains work), our behavior (i.e. make the same mistakes as humans), our capabilities (i.e. do stuff previously only humans could do), our cognitive functions (e.g. ability to perceive, learn, reason, etc.), or the principle of intelligence/rationality?

**Source:** https://www.reddit.com/r/artificial/comments/4rzp2m/is_anyone_in_aimachine_learning_community_working/d55mwjh

# Unformatted
intelligence in itself is nothing more than 0 and 1
Given an input stream of seemingly random 0 and 1, "an intelligent being" will start to output its own stream of 0 and 1 in return
this generates communication between two or more entities
at this point the problem is that communication is nothing more than junk sent between entities
what can then happen though is that an entity could start outputting a stream which would have taken an input stream through a function f(x) (for example f(x) = x, f(x) = !x)
through this function it would start to look as if the entity had some form of intelligence
thus, the logical next step would be more complex functions acting on the input stream, functions accepting multiple input streams at once, functions accepting its own output as input stream (feedback)
an entity then slowly starts to look like a neural network implementation where each node is given a function, some form of activator function (which will trigger the function based on input or stay idle/untriggered otherwise)

one of the things I wonder about is how functions get generated and why they decide to stay in memory and begin to be executed
there is also the question of how an entity can determine that is output is being used by another entity, and if that former entity is basing its own thoughts on the latter entity, how will it be able to determine if that it is the case? How would an entity be able to determine it is "discussing" with a clone of itself? Would it have to execute all its own functions against the clone to figure that out? If the clone is a superset of the initial entity, that clone would implement all of the original entity functions yet implement additional functions
how would the entity be able to probe for missing or additional (or different) functions?

how is an entity suppose to evolve from what is a seemingly random input stream and start recognizing patterns?

The brain is nothing more than processing. Memory is simply a side effect of its processing capabilities (part of its programming if you will). Can we consider programming/code to be memory in some way? Similarly, DNA would be our program?


# See also

# References
