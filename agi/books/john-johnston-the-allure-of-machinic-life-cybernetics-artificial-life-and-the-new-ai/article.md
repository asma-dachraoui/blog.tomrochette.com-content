---
title: John Johnston - The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI - 2008
created: 2017-10-12
taxonomy:
  category: [Artificial General Intelligence]
  status: in progress
---

## Context

## Learned in this study

## Things to explore
* Is it possible to create some sort of hierarchy or organization of all life by looking at DNA as a form of code shared by all organisms?
* What is the smallest organism that has DNA?
	* https://biology.stackexchange.com/questions/1382/which-organism-has-the-smallest-genome-length
* What comes after machines?

# Overview

# Notes
## Introduction
### Liminal Machines
* In strong theories of ALife these machines are understood not simply to simulate life but to realize it, by instantiating and actualizing its fundamental principles in another medium or material substrate
* These forms of machinic life are characterized not by an exact imitation of natural life but by complexity of behavior
* Thinking in terms of the complexity of automata, whether natural or artificial, rather than in terms of a natural biological hierarchy is part of the legacy of cybernetics
* While contemporary biologists have reached no consensus on a definition of life, there is wide agreement that two basic processes are involved: some kind of metabolism by which energy is extracted from the environment, and reproduction with a hereditary mechanism that will evolve adaptations for survival
* Theoretical biologist Stuart Kauffman has suggested that thinking of the development of an organism as a program consisting of serial algorithms is limiting and that a "better image of the genetic program - as a parallel distributed regulatory network - leads to a more useful theory"
* The genetic program works by means of a parallel and highly distributed rather than serial and centrally controlled computational mechanism
	* This echoes the observation made by Christopher Langton that computation in nature is accomplished by large numbers of simple processors that are only locally connected
* A technical system forms when a technical evolution stabilizes around a point of equilibrium concretized by a particular technology

### The Computational Assemblage
* A computational assemblage comprises a material computational device set up or programmed to process information in specific ways together with a specific discourse that explains and evaluates its function, purpose, and significance
* Computational assemblages give rise to new ways of thinking about the relationship between physical processes (most importantly, life processes) and computation, or information processing
* Every isolated determinate dynamic system obeying unchanging laws will develop "organisms" that are adapted to their "environment"
* Wolfram presented a seminal demonstration of how the dynamic behavior of cellular automata falls into four distinct universality classes:
	* One that halts after a reasonable number of computations
	* one that falls into a repetitive loop or periodic cycle
	* One that generates a chaotic, random mess
	* One (the most complex) that produces persistent patterns that interact across the local spaces of the grid

### Narratives of Machinic Life
* Our human capacity as toolmakers (homo faber) has made us the vehicle and means of realization for new forms of machinic life
* This strand of thinking has given rise to two conflicting cultural narratives, the adversarial and the symbiotic
	* Adversarial: Human beings will completely lose control of the technical system, as silicon life in the form of computing machines performs what Hans Moravec calls a "genetic take-over" from carbon life
	* Symbiotic: Human beings will gradually merge with the technical system that defines and shapes the environment in a transformative symbiosis that will bring about and characterize the advent of the posthuman
* The real questions are how global properties and behaviors emerge in a system from the interactions of computational "primitives" that behave according to simple rules and how these systems are enchained in dynamic hierarchies that allow complexity to build on complexity
* One significant current in ALife research asserts that complexity (or complex adaptive systems) rather than "life" (and thus the opposition to nonlife) is the conceptually more fruitful framework

### Lamarckian Evolution or Becoming Machinic
* Lamarck's theory: acquired traits are passed down to subsequent generations through hereditary mechanisms
* Artifacts with similar purposes may be designed to very different specifications and chosen for very different reasons
* Herbert Spencer's concept of evolution: evolution is a process giving rise to increasing differentiation (specialization of functions) and integration (mutual inter-dependence and coordination of function of the structurally differentiated parts)
* Computationalism: all physical processes can be viewed or understood as computations
* One widely accepted example is the view that evolution itself is simply a vast computational process, a slow but never-ending search through a constantly changing "real" fitness landscape for better-adapted forms

## 1 Cybernetics and the New Complexity of Machines
* The living organism: a heat engine, burning glucose or glycogen or starch, fat, and proteins into carbon dioxide, water and urea
* The organism's body is very far from a conservative system, and that its component parts work in an environment where the available power is much less limited than we have taken it to be
* Whereas for Shannon information measures uncertainty, or entropy, for Wiener it measures a gain in certainty; information, therefore, he considered to be a measure of negative entropy, or "negentropy"

### Conceptualizing the New Machine
* Ashby's approach to machines: a machine is that which behaves in a machinelike way, namely, that its internal state, and the state of its surroundings, defines uniquely the next it will go to

### Von Neumann's Self-Reproducing Automata
* How can the differences that underlie the logic of organization in biological as opposed to artificial entities be used to build more reliable machines? More specifically, how can unreliable components be organized to become highly reliable for a machine or automaton as a whole? What are the conditions that would enable simple automata - understood as information-processing machines that exhibit self-regulation in interaction with the environment - to produce more complex automata?
* When we talk mathematics, we may be discussing a secondary language, built on the primary language truly used by the nervous system
* For simple automata, it is easier to describe the behavior itself than exactly how this behavior is produced or effectuated
* Above a certain threshold of complexity the description of the structure would be simpler than a description of the behavior
* First, the logic of automata would have to be continuous rather than discrete, analytical rather than combinatorial
* Second, it would have to be a "probabilistic logic which would handle component malfunction as an essential and integral part of automata operation"
* Third, it would most likely have to draw on the resources of thermodynamics and information theory
* By his own calculations, the neurons in the brain are some 5,000 times slower than the vacuum tubes used as switching devices in the first electronic calculators, yet they are far more reliable. This is simply because they are far more numerous, and their connections more complicated
* This flexibility (having a high degree of error tolerance), von Neumann speculates, probably requires an "ability of the automaton to watch itself and reorganize itself"
* Turning to the problem of error, von Neumann introduces the idea of "multiplexing," that is, of carrying a single message simultaneously on multiple lines, and demonstrates statistically that by using large bundles of lines any degree of reliability for a circuit can be insured

### Ross Ashby's Homeostat
* When changes in the environment occur, an organism must adapt itself to the new conditions in order to survive
* In Ashby's approach, the environment - designated as E - is also a transducer, or operator, in the sense that "it converts whatever action comes from the organism into some effect that goes back to the organism"
* The brain of the organism must therefore act as an inverse operator $E^{-1}$ capable of reacting in such a way that the environmental disturbance is followed by an action that returns the organism to the proper values of its own variables

# See also

# References
* Johnston, John. "The Allure of Machinic Life: Cybernetics." Artificial Life and the New AI. Cambridge, Mss & London, The MIT Press (2008).