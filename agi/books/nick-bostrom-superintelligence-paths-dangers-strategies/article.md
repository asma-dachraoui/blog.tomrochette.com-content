---
title: Nick Bostrom - Superintelligence: Paths, Dangers, Strategies - 2014
created: 2017-08-25
taxonomy:
  category: [Artificial General Intelligence]
  status: in progress
---

## Context

## Learned in this study

## Things to explore
* How would a superintelligence be different than human society?
	* In itself, a society or a company is a sublinear arrangement of human minds (that is, the addition of new people to the group scales sublinearly)
* Bostrom uses the term "make the detonation survivable", which reminds me of the development of the atomic bomb. How different would intelligence explosion be from the development of the atomic bomb?
* Given infinite computing power, how would one be able to detect/recognize intelligent behavior from programs (out of all the randomly generated programs)?
* Is it possible to define some sort of "unit of intellectual work"?
* Should we expect generalists+specialists collective systems to outperform generalists only (monolithic) systems?
* What type of invention, similar to the print press, electricity or the Internet would lead to an intellectual revolution?
* Given that there is an infinity of possible ways for a superintelligence creator to prevent the superintelligence from gaining decisive strategic advantage and due to Bayesian formalism, should we assume that an agent should never attempt to gain decisive strategic advantage? (this sounds like some form of dilemma/paradox)

# Overview

# Notes
* In regular font are notes/excerpts from the book
* In italic font are my comments

## Preface
* The control problem: the problem of how to control what the superintelligence would do
* *Creating a superintelligence seems to imply to the author that we can only create one and not many, as we are many human individuals*
	* *Maybe it has to do with the idea of superintelligence hierarchies, where a superintelligence will dominate all others*

## Chapter 1 - Past developments and present capabilities
### Seasons of hope and despair
* To overcome the combinatorial explosion, one needs algorithms that exploit structure in the target domain and take advantage of prior knowledge by using heuristic search, planning, and flexible abstract representations

### State of the art
* AI-complete: the difficulty of solving these problems is essentially equivalent to the difficulty of building generally human-level intelligent machines. In other words, if somebody were to succeed in creating an AI that could understand natural language as well as a human adult, they would in all likelihood also either already have succeeded in creating an AI that could do everything else that a human intelligence can do, or they would be but a very short step from such a general capability

## Chapter 2 - Paths to superintelligence
* We can tentatively define a superintelligence as any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest

### Artificial intelligence
* We can discern some general features of the kind of system that would be required
* It now seems clear that a capacity to learn would be an integral feature of the core design of a system intended to attain general intelligence
* The same holds for the ability to deal effectively with uncertainty and probabilistic information
* Some faculty for extracting useful concepts from sensory data and internal states, and for leveraging acquired concepts into flexible combinatorial representations for use in logical and intuitive reasoning
* Another way of arguing for the feasibility of artificial intelligence is by pointing to the human brain and suggesting that we could use it as a template for a machine intelligence
* One can distinguish different versions of this approach based on how closely they propose to imitate biological brain functions
	* At one extreme - that of very close imitation - we have the idea of whole brain emulation
	* At the other extreme are approaches that take their inspiration from the functioning of the brain but do not attempt low-level imitation
* Since there is a limited number - perhaps a very small number - of distinct fundamental mechanisms that operate in the brain, continuing incremental progress in brain science should eventually discover them all
* An artificial intelligence need not much resemble a human mind. AIs could be - indeed, it is likely that most will be - extremely alien
* We should expect that they will have very different cognitive architectures than biological intelligences, and in their early stages of development they will have very different profiles of cognitive strengths and weaknesses

### Whole brain emulation
* In whole brain emulation (also known as "uploading"), intelligent software would be produced by scanning and closely modeling the computational structure of a biological brain
* Achieving whole brain emulation requires the accomplishment of the following steps
	* A sufficiently detailed scan of a particular human brain is created
	* The raw data from the scanners is fed to a computer for automated image processing to reconstruct the three-dimensional neuronal network that implemented cognition in the original brain
	* The neurocomputational structure resulting from the previous step is implemented on a sufficiently powerful computer
* The whole brain emulation path does not require that we figure out how human cognition works or how to program an artificial intelligence
* It requires only that we understand the low-level functional characteristics of the basic computational elements of the brain
* Whole brain emulation does require some rather advanced enabling technologies. There are three key prerequisites:
	* Scanning: high-throughput microscopy with sufficient resolution and detection of relevant properties
	* Translation: automated image analysis to turn raw scanning data into an interpreted three-dimensional model of relevant neurocomputational elements
	* Simulation: hardware powerful enough to implement the resultant computational structure
* The aim is not to create a brain simulation so detailed and accurate that one could use it to predict exactly what would have happened in the original brain if it had been subjected to a particular sequence of stimuli. Instead, the aim is to capture enough of the computationally functional properties of the brain to enable the resultant emulation to perform intellectual work
* Knowing merely which neurons are connected with which is not enough. TO create a brain emulation one would also need to know which synapses are excitatory and which are inhibitory; the strength of the connections; and various dynamical properties of the axons, synapses, and dendritic trees

### Biological cognition
* A third path to greater-than-current-human intelligence is to enhance the function of biological brains
* In principle, this could be achieved without technology, through selective breeding
* It seems implausible, on both neurological and evolutionary grounds, that one could by introducing some chemical into the brain of a healthy person spark a dramatic rise in intelligence
* Manipulation of genetics will provide a more powerful set of tools than psychopharmacology
* The problem with sequential selection, of course, is that it takes longer. If each generational step takes twenty or thirty years, then even just five successful generations would push us well into the twenty-second century
* One intervention that becomes possible when human genomes can be synthesized is genetic "spell-checking" of an embryo
* Each of us currently carries a mutational load, with perhaps hundreds of mutations that reduce the efficiency of various cellular processes
* With gene synthesis we could take the genome of an embryo and construct a version of that genome free from the genetic noise of accumulated mutations
* Three conclusions
	* at least weak forms of superintelligence are achievable by means of biotechnological enhancements
	* the feasibility of cognitively enhanced humans adds to the plausibility that advanced forms of machine intelligence are feasible
	* when we consider scenarios stretching significantly into the second half of this century and beyond, we must take into account the probable emergence of a generation of genetically enhanced populations, with the magnitude of enhancement escalating rapidly over subsequent decades

### Brain-computer interfaces
* Although the possibility of direct connections between humans brains and computers has been demonstrated, it seems unlikely that such interfaces will be widely used as enhancements any time soon
* To begin with, there are significant risks of medical complications -  infections, electrode displacement, hemorrhage, and cognitive decline - when implanting electrodes in the brain
* The second reason to doubt superintelligence will be achieved through cyborgization is that enhancement is likely to be far more difficult than therapy
* Even if there were an easy way of pumping more information into our brains, the extra data inflow would do little to increase the rate at which we think and learn unless all the neural machinery necessary for making sense of the data were similarly upgraded
* Keeping our machines outside of our bodies also makes upgrading easier
* The rate-limiting step in human intelligence is not how fast raw data can be fed into the brain but rather how quickly the brain can extract meaning and make sense of the data
* Perhaps it will be suggested that we transmit meanings directly, rather than package them into sensory data that must be decoded by the recipient. There are two problems with this
	* Brains, by contrast to the kinds of program we typically run on our computers, do not use standardized data storage and representation formats. Rather, each brain develops its own idiosyncratic representations of higher-level content
	* Creating the required interface (to read/write from billions of individually addressable neurons) is probably an AI-complete problem
* One hope for the cyborg route is that the brain, if permanently implanted with a device connecting it to some external resource, would over time learn an effective mapping between its own internal cognitive states and the inputs it receives from, or the outputs accepted by, the device
	* Then the implant itself would not need to be intelligent; rather, the brain would intelligently adapt to the interface, much as the brain of an infant gradually learns to interpret the signals arriving from receptors in its eyes and ears

### Networks and organizations
* The idea here is not that this would enhance the intellectual capacity of individuals enough to make them superintelligent, but rather that some system composed of individuals thus networked and organized might attain a form of superintelligence
* In general terms, a system's collective intelligence is limited by the abilities of its member minds, the overheads in communicating relevant information between them, and the various distortions and inefficiencies that pervade human organizations
* If communication overheads are reduced (including not only equipment costs but also response latencies, time and attention burdens, and other factors), then larger and more densely connected organizations become feasible
* Could the Internet become something more than just the backbone of a loosely integrated collective superintelligence - something more like a virtual skull housing an emerging unified super-intellect

## Chapter 3 - Forms of superintelligence
* We use the term "superintelligence" to refer to intellects that greatly outperform the best current human minds across many very general cognitive domains
* We will differentiate between three forms:
	* speed superintelligence
	* collective superintelligence
	* quality superintelligence

### Speed superintelligence
* A speed superintelligence is an intellect that is just like a human mind but faster
* Speed superintelligence: A system that can do all that a human intellect can do, but much faster
* An emulation operating at a speed of ten thousand times that of a biological brain would be able to read a book in a few seconds and write a PhD thesis in a afternoon. With a speedup factor of a million, an emulation could accomplish an entire millenium of intellectual work in one working day
* Suppose your mind ran at 10000x
	* Because of this apparent time dilation of the material world, a speed superintelligence would prefer to work with digital objects
	* Alternatively, it could interact with the physical environment by means of nanoscale manipulators, since limbs at such small scales could operate faster than macroscopic appendages
* The speed of light becomes an increasingly important constraint as minds get faster, since faster minds face greater opportunity costs in the use of their time for traveling or communicating over long distances

### Collective superintelligence
* Collective superintelligence: A system composed of a large number of smaller intellects such that the system's overall performance across many very general domains vastly outstrips that of any current cognitive system
* Collective intelligence excels at solving problems that can be readily broken into parts such that solutions to sub-problems can be pursued in parallel and verified independently
* A system's collective intelligence could be enhanced by expanding the number or the quality of its constituent intellects, or by improving the quality of their organization
* The definition does not even imply that the more collectively intelligent society is wiser
* We can think of wisdom as the ability to get the important things approximately right
* We should recognize that there can exist instrumentally powerful information processing systems - intelligent systems - that are neither inherently good nor reliably wise
* A collective superintelligence could, after gaining sufficiently in integration, become a "quality superintelligence"

### Quality superintelligence
* Quality superintelligence: A system that is at least as fast as a human mind and vastly qualitatively smarter
* We can expand the range of our reference points by considering nonhuman animals, which have intelligence of lower quality
* Nonhuman animals lack complex structured language; they are capable of no or only rudimentary tool use and tool construction; they are severely restricted in their ability to make long-term plans; and they have very limited abstract reasoning ability
* The concept of quality superintelligence: it is intelligence of quality at least as superior to that of human intelligence as the quality of human intelligence is superior to that of elephants', dolphins', or chimpanzees'
* A second way to illustrate the concept of quality superintelligence is by noting the domain-specific cognitive deficits that can afflict individual humans, particularly deficits that are not caused by general dementia or other conditions associated with wholesale destruction of the brain's neurocomputational resources
* Such examples show that normal human adults have a raneg of remarkable cognitive talents that are not simply a function of possessing a sufficient amount of general neural processing power or even a sufficient amount of general intelligence: specialized neural circuitry is also needed

### Direct and indirect reach
* We might say that speed superintelligence excels at tasks requiring the rapid execution of a long series of steps that must be performed sequentially while collective superintelligence excels at tasks admitting of analytic decomposition into parallelizable sub-tasks and tasks demanding the combination of many different perspectives and skill sets
* In some domains, quantity is a poor substitute for quality
* If we widen our purview to include superintelligent minds, we must countenance a likelihood of there being intellectual problems solvable only by superintelligence and intractiable to any ever-so-large collective of non-augmented humans
* We cannot clearly see what all these problems are, but we can characterize them in general terms. They would tend to be problems involving multiple complex interdependencies that do not permit of independently verifiable solution steps: problems that therefore cannot be in piecemeal fashion, and that might require qualitatively new kinds of understanding or new representational frameworks that are too deep or too complicated for the current edition of mortals to discover or use effectively

### Sources of advantage for digital intelligence
* Hardware advantages
	* Speed of computational elements
	* Internal communication speed
	* Number of computational elements
	* Storage capacity
	* Reliability, lifespan, sensors, etc.
* Software advantages
	* Editability
	* Duplicability
	* Goal coordination
	* Memory sharing
	* New modules, modalities, and algorithms

## Chapter 4 - The kinetics of an intelligence explosion
### Timing and speed of the takeoff
* If and when such machine is developed, how long will it be from then until a machine becomes radically superintelligent?
* We can distinguish three classes of transition scenarios, based on their steepness
	* Slow: A slow takeoff is one that occurs over some long temporal interval, such as decades or centuries
	* Fast: A fast takeoff occurs over some short temporal interval, such as minutes, hours, or days
	* Moderate: A moderate takeoff is one that occurs over some intermediary temporal interval, such as months or years
* We can conceive the rate of increase in a system's intelligence as a (monotonically increasing) function of two variables: the amount of "optimization power" , or quality-weighted design effort, that is being applied to increase the system's intelligence, and the responsiveness of the system to the application of a given amount of such optimization power
$$
\text{Rate of change in intelligence} = \frac{\text{Optimization power}}{\text{Recalcitrance}}
$$
* A system's recalcitrance might also vary depending on how much the system has already been optimized
	* Often, the easiest improvements are made first, leading to diminishing returns as low-hanging fruits are depleted

### Recalcitrance
#### Emulation and AI paths
* The path toward artificial intelligence may feature no obvious milestone or early observation point. It is entirely possible that the quest for artificial intelligence will appear to be lost in dense jungle until an unexpected breakthrough reveals the finishing line in a clearing just a few short steps away
* It is quite possible that recalcitrance falls when a machine reaches human parity
* Suppose an AI is composed of two subsystems, one possessing domain-specific problem-solving techniques, the other possessing general-purpose reasoning ability. It could be the case that while the second subsystem remains below a certain capacity threshold, it contributes nothing to the system's overall performance, because the solutions it generates are always inferior to those generated by the domain-specific subsystem. Suppose now that a small amount of optimization power is applied to the general-purpose subsystem and that this produces a brisk rise in the capacity of that subsystem. At first, we observe no increase in the overall system's performance, indicating that recalcitrance is high. Then, once the capacity of the general-purpose subsystem crosses the threshold where its solutions start to beat those of the domain-specific subsystem, the overall system's performance suddenly beings to improve at the same brisk pace as the general-purpose subsystem, even as the amount of optimization power applied stays constant: the system's recalcitrance has plummeted
* It is also possible that our natural tendency to view intelligence from an anthropocentric perspective will lead us to underestimate improvements in sub-human systems, and thus to overestimate recalcitrance
* A system's intellectual problem-solving capacity can be enhanced not only by making the system cleverer but also by expanding what the system knows
* In order to tap the full potential of fast content accumulation, however, a system needs to have a correspondingly large memory capacity. There is little point in reading an entire library if you have forgotten all about the aardvark by the time you get to the abalone

### Optimization power and explosivity
* Two phases
	* The first phase begins with the onset of the takeoff, when the system reaches the human baseline for individual intelligence. Most of the optimization power applied to the system still comes from outside the system, either from the work of programmers or engineers
	* A second phase will begin if at some point the system has acquired so much capability that most of the optimization power exerted on it comes from the system itself

## Chapter 5 - Decisive strategic advantage
* If the takeoff is fast then it is unlikely that two independent projects would be taking off concurrently: almost certainly, the first project would have completed its takeoff before any other project would have started its own
* If the takeoff is slow then there could plausibly be multiple projects undergoing takeoffs concurrently, so that although the projects would by the end of the transition have gained enormously in capability, there would be no time at which any project was far enough ahead of the others to give it an overwhelming lead
* If a project did obtain a decisive strategic advantage, would it use it to suppress competitors and form a singleton (a world order in which there is at the global level a single decision-making agency)?

### Will the frontrunner get a decisive strategic advantage?
* One factor influencing the width of the gap between frontrunner and followers is the rate of diffusion of whatever it is that gives the leader a competitive advantage
	* A frontrunner might find it difficult to gain and maintain a large lead if followers can easily copy the frontrunner's ideas and innovations
* The mere demonstration of the feasibility of an invention can also encourage others to develop it independently

### How large will the successful project be?
* The likelihood of the final breakthrough being made by a small project increases if most previous progress in the field has been published in the open literature or made available as open source software

#### Monitoring
* Projects designed from the outset to be secret could be more difficult to detect. An ordinary software development project could serve as a front

#### International collaboration
* A country that believed it could achieve a breakthrough unilaterally might be tempted to do it alone rather than subordinate its efforts to a joint project. A country might refrain from joining an international collaboration from fear that other participants might siphon off collaboratively generated insights and use them to accelerate a covert national project

### From decisive strategic advantage to singleton
* Many factors might dissuade a human organization with a decisive strategic advantage from creating a singleton. These include non-aggregative or bounded utility functions, non-maximizing decision rules, confusion and uncertainty, coordination problems, and various costs associated with a takeover
* Human individuals and human organizations typically have preferences over resources that are not well represented by an "unbounded aggregative utility function." A human will typically not wager all her capital for a fifty-fifty chance of doubling it
* Humans and human-run organizations may also operate with decision processes that do not seek to maximize expected utility

## Chapter 6 - Cognitive superpowers
### Functionalities and superpowers
* The most essential characteristic of a seed AI, aside from being easy to improve (having low recalcitrance), is being good at exerting optimization power to amplify a system's intelligence

### An AI takeover scenario
* How could a superintelligence achieve the goal of world domination?
	* Pre-criticality phase
	* Recursive self-improvement phase
	* Covert preparation phase
	* Overt implementation phase

### Power over nature and agents
* An agent's ability to shape humanity's future depends not only on the absolute magnitude of the agent's own faculties and resources, but also on the relative magnitude of its capabilities compared with those of other agents with conflicting goals
* In a situation where there are no competing agents, the absolute capability level of a superintelligence, so long as it exceeds a certain minimal threshold, does not matter much, because a system starting out with some sufficient set of capabilities could plot a course of development that will let it acquire any capabilities it initially lacks
* The wise-singleton sustainability threshold: A capability set exceeds the wise-singleton threshold if and only if a patient and existential risk-savvy system with that capability set would, if it faced no intelligent opposition or competition, be able to colonize and re-engineer a large part of the accessible universe

## Chapter 7 - The superintelligent will
* Intelligence and final goals are independent variables: any level of intelligence could be combined with any final goal

### The relation between intelligence and motivation
* Intelligence and motivation are in a sense orthogonal: we can think of them as two axes spanning a graph in which each point represents a logically possible artificial agent
* The orthogonality thesis: Intelligence and final goals are orthogonal: more or less any level of intelligence could in principle be combined with more or less any final goal
* There are at least three directions from which we can approach the problem of predicting superintelligent motivation:
	* Predictability through design
	* Predictability through inheritance
	* Predictability through convergent instrumental reasons

### Instrumental convergence
* There are likely some instrumental goals likely to be pursued by almost any intelligent agent, because there are some objectives that are useful intermediaries to the achievement of almost any final goal
* The instrumental convergence thesis: Several instruments values can be identified which are convergent in the sense that their attainment would increase the chances of the agent's goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by a broad spectrum of situated intelligent agents

### Goal-content integrity
* If an agent retains its present goals into the future, then its present goals will be more likely to be achieved by its future self
* There are situations in which an agent can best fulfill its final goals by intentionally changing them. Such situations can arise when any of the following factors is significant:
	* Social signaling
	* Social preferences
	* Preferences concerning own goal content
	* Storage costs

### Cognitive enhancement
* Improvements in rationality and intelligence will tend to improve an agent's decision-making, rendering the agent more likely to achieve its final goals
* One would therefore expect cognitive enhancement to emerge as an instrumental goal for a wide variety of intelligent agents
* For similar reasons, agents will tend to instrumentally value many kinds of information
* An agent that has access to reliable expert advice may have little need for its own intelligence and knowledge
* If intelligence and knowledge come at a cost, such as time and effort expended in acquisition, or increased storage or processing requirements, then the agent might prefer less knowledge and less intelligence

### Technological perfection
* A software agent might place an instrumental value on more efficient algorithms that enable its mental functions to run faster on given hardware

### Resource acquisition
* Human beings tend to seek to acquire resources sufficient to meet their basic need
* But people usually seek to acquire resources far beyond this minimum level
* A great deal of resource accumulation is motivated by social concerns - gaining status, mates, friends, and influence, through wealth accumulation and conspicuous consumption

## Chapter 8 - Is the default outcome doom?
### Existential catastrophe as the default outcome of an intelligence explosion?
* Absent of a special effort, the first superintelligence may have a random or reductionist final goal
* The treacherous turn: While weak, an AI behaves cooperatively (increasingly so, as it gets smarter). When the AI gets sufficiently strong - without warning or provocation - it strikes, forms a singleton, and begins directly to optimize the world according to the criteria implied by its final values
* An AI might calculate that if it is terminated, the programmers who built it will develop a new and somewhat different architecture, but one that will be given a similar utility function. In this case, the original AI may be indifferent to its own demise, knowing that its goals will continue to be pursued in the future

### Malignant failure modes
* There are ways of failing that we might term "malignant" in that they involve an existential catastrophe
* One feature of a malignant failure is that it eliminates the opportunity to try again
* Another feature of malignant failure is that it presupposes a great deal of success: only a project that got a great number of things right could succeed in building a machine intelligence powerful enough to pose a risk of malignant failure

#### Perverse instantiation
* A superintelligence discovering some way of satisfying the criteria of its final goal that violates the intentions of the programmers who defined the goal

#### Infrastructure profusion
* The phenomenon where an agent transforms large parts of the reachable universe into infrastructure in the service of some goal, with the side effect of preventing the realization of humanity's axiological potential
* Unless the AI's motivation system is of a special kind, or there are additional elements in its final goal that penalize strategies that have excessively wide-ranging impacts on the world, there is no reason for the AI to cease activity upon achieving its goal. On the contrary: if the AI is a sensible Bayesian agent, it would never assign exactly zero probability to the hypothesis that it has not yet achieved its goal
* The claim here is that there is no possible way to avoid this failure mode
* Might we avoid this malignant outcome if instead of a maximizing agent we build a satisficing agent, one that simply seeks to achieve an outcome that is "good enough" according to some criterion, rather than an outcome that is as good as possible?

#### Mind crime
* In mind crime, the side effect is not external to the AI; rather, it concerns what happens within the AI itself (or within the computational processes it generates)
* A machine superintelligence could create internal processes that have moral status
	* One can imagine scenarios in which an AI creates trillions of such conscious simulations, perhaps in order to improve its understanding of human psychology and sociology. These simulations might be placed in simulated environments and subjected to various stimuli, and their reactions studied. Once their informational usefulness has been exhausted, they might be destroyed

## Chapter 9 - The control problem
### Two agency problems
* Could we engineer the initial conditions of an intelligence explosion so as to achieve a specific desired outcome, or at least to ensure that the result lies somewhere in the class of broadly acceptable outcomes?
	* How can the sponsor of a project that aims to develop superintelligence ensure that the project, if successful, produces a superintelligence that would realize the sponsor's goal?
* The first principal-agent problem: Whenever some human entity ("the principal") appoints another ("the agent") to act in the former's interest (Human vs Human, Sponsor -> Developer)
* The second principal-agent problem (the control problem): In this case, the agent is not a human agent operating on behalf of a human principal. Instead, the agent is the superintelligent system (Human vs Superintelligence, Project -> System)

### Capability control methods
* Capability control methods seek to prevent undesirable outcomes by limiting what the superintelligence can do

#### Boxing methods
* Physical containment aims to confine the system to a "box," i.e. to prevent the system from interacting with the external world otherwise than via specific restricted output channels
* For extra security, the system should be placed in a metal mesh to prevent it from transmitting radio signals, which might otherwise offer a means of manipulating electronic objects such as radio receivers in the environment
* Physical containment has several advantages
	* It is easy to implement
	* It can be applied to many machine intelligence architectures, even ones that were not initially designed with safety as an objective
	* It can be used in combination with most other control methods
	* It seems unlikely to go wrong by backfiring: while it might fail to ensure safety, it is unlikely to cause a catastrophe that would not otherwise have occurred
* The main disadvantage with physical confinement is that it reduces the functionality of the superintelligence
* Another concern is that it might encourage a false sense of security, though this is avoidable if we regard physical confinement as icing on the cake rather than the main substance of our precautions
* Informational containment aims to restrict what information is allowed to exit the box
* An obvious informational containment method is to bar the system from accessing communications networks
* The limiting case of the boxing approach would be a system kept in complete physical and informational isolation
* Even if achievable, however, such a system would be rather useless since it would have no effect on the external world
* It might perhaps be thought that some scientific purpose could be served by creating a superintelligence and keeping it in isolation: by studying a self-contained model system, one could learn about its internal dynamics and its behavior patterns
	* But this would be an error. As soon as the system is observed, it ceases to be informationally isolated
* An AI anticipating that it might be observed could strategically adopt behaviors designed to influence the hypothesized observers

#### Incentive methods
* Incentive methods involve placing an agent in an environment where it finds instrumental reasons to act in ways that promote the principal's interests
* It presupposes a balance of power: legal or economic sanctions cannot restrain an agent that has a decisive strategic advantage
* By relying on social integration to solve the control problem, the principal risks sacrificing a large portion of his potential influence
* A better alternative might be to combine the incentive method with the use of motivation selection to give the AI a final goal that makes it easier to control
* A problem with the incentive scheme is that it presupposes that we can tell whether the outcomes produced by the AI are in our interest

#### Stunting
* Limit the system's intellectual faculties or its access to information
* Even without any designated knowledge base at all, a sufficiently superior mind might be able to learn much by simply introspecting on the workings of its own psyche - the design choices reflected in its source code, the physical characteristics of its circuitry

#### Tripwires
* A tripwire is a mechanism that performs diagnostic tests on the system (possibly without its knowledge) and effects a shutdown if it detects signs of dangerous activity
* Tripwires differ from incentive methods in that they do not rely on the system being aware of the consequences of engaging in forbidden activities

### Motivation selection methods
* Motivation selection methods seek to prevent undesirable outcomes by shaping what the superintelligence wants to do
* Motivation selection can involve explicitly formulating a goal or set of rules to be followed (direct specification) or setting up the system so that it can discover an appropriate set of values for itself by reference to some implicitly or indirectly formulated criterion (indirect normativity)
* One option in motivation selection is to try to build the system so that it would have modest, non-ambitious goals (domesticity)
* An alternative to creating a motivation system from scratch is to select an agent that already has an acceptable motivation system and then augment that agent's cognitive powers to make it superintelligent, while ensuring that the motivation system does not get corrupted in the process (augmentation)

#### Direct specification
* The approach comes in two versions, rule-based and consequentialist
* Involves trying to explicitly define a set of rules or values that will cause even a free-roaming superintelligent AI to act safely and beneficially
* Difficulties in determining which rules or values we would wish the AI to be guided by and the difficulties in expressing those rules or values in computer-readable code
* The traditional illustration of the direct rule-based approach is the "three laws of robotics"
	* A robot may not injure a human being or, through inaction, allow a human being to come to harm
	* A robot must obey any order given to it by human beings, except where such orders would conflict with the First Law
	* A robot must protect its own existence as long as such protection does not conflict with the First or Second Law
* Everything is vague to a degree you do not realize till you have tried to make it precise -Bertrand Russell

#### Domesticity
* One could try to design an AI such that it would function as a question-answering device

#### Indirect normativity
* The basic idea is that rather than specifying a concrete normative standard directly, we specify a process for deriving a standard. We then build the system so that it is motivated to carry out this process and to adopt whatever standard the process arrives at
* Indirect normativity is a very important approach to motivation selection. Its promise lies in the fact that it could let us offload to the superintelligence much of the difficult cognitive work required to carry out a direct specification of an appropriate final goal

#### Augmentation
* The idea is that rather than attempting to design a motivation system de novo, we start with a system that already has an acceptable motivation system, and enhance its cognitive faculties to make it superintelligent
* The attractiveness of augmentation may increase in proportion to our despair at the other approaches to the control problem

## Chapter 10 - Oracles, genies, sovereigns, tools
### Oracles
* An oracle is a question-answering system
* It might accept questions in a natural language and present its answers as text
* Even an untrustworthy oracle could be useful
	* We could ask an oracle questions of a type for which it is difficult to find the answer but easy to verify whether a given answer is correct
	* If it is expensive to verify answers, we can randomly select a subset of the oracle's answers for verification. If they are all correct, we can assign a high probability to most of the other answers also being correct

### Genies and sovereigns
* A genie is a command-executing system: it receives a high-level command, carries it out, then pauses to await the next command
* A sovereign is a system that has an open-ended mandate to operate in the world in pursuit of broad and possibly very long-range objectives
* The ideal genie would be a super-butler rather than an autistic savant
* One option would be to try to build a genie such that it would automatically present the user with a prediction about salient aspects of the likely outcomes of a proposed command, asking for confirmation before proceeding
* The difference between oracles, genies and sovereigns comes down to alternative approaches to the control problem
* Safety order: sovereigns < genies < oracles

# See also

# References
* Bostrom, Nick. Superintelligence: Paths, dangers, strategies. OUP Oxford, 2014.