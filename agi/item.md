---
title: Artificial General Intelligence
created: 2015-07-04
taxonomy:
    category: [artificial general intelligence]
    status: in progress
---

# Artificial General Intelligence

[TOC]

-----

To limit the dangers of AI, contain them in a physical body (which can be destroyed).
By extension, this means that human should be able to extend their reach through externalization, that is, provide APIs (function library) of their thought process so that others may query that process.

-----

# Memory/thinking as a state machine

Memory is the construction of a finite?/nondeterministic? state machine
Thought is the process of going through the state machine
Memory (the state machine) can reconfigure itself based on input

- Reconfigure itself how?
- How often does it reconfigure itself?

In a sense, a state machine with probabilistic paths resembles a neural network. **(resembles how? and what is different between a FSM and a NN?)**
In order to process ideas quickly, the state machine must have a central node with a immense breadth so that it will easily trigger the appropriate path based on the current input. Once the input is processed, the state returns to this central node which awaits further input. This main process is basically consciousness.

-----

If the big bang is true, then we are a simulation. Every thought, action, atom current property is defined in a table at time t = x.

-----

Concepts are basically numbers
Everything can be represented by a number (basically the concept of primary key -> set of data)
We learn to associate images, sounds, experiences (samples) with a specific concept (number)

-----

<pre>
<tomzx> well, my understanding of turing so far is that you can represent pretty much anything as a number
<tomzx> except those non-computable numbers
<tomzx> so every word can be represented as a number, phrase (order of words) as a number, documents as a number, thoughts as a number, etc.
<tomzx> basically everything can be labelled
<tomzx> then you can "easily" say A <-> B
<tomzx> in the sense that the entity represented by A is related to the entity represented by B
<tomzx> although I don't think that gets us very far
</pre>

-----

A super-intelligent AI decides that the best use of matter/energy in its vicinity is to nano-engineer the raw materials necessary for its mind to expand. It sends out quadrillions of self-replicating probes in a spherical region, programmed to reduce all matter they encounter into the "smart" matter which the AI can use as its brain. 2500 years later, give or take, there is a massive 5000 light year sphere of "empty" space. It's actually all cognitive matter.

=> is this the reduction to a Turing machine? space is basically just information encoded

-----

I believe intelligence emerges from complexity. By stacking simple concepts onto one another, intelligence appears.

-----

Reading a word is basically triggering letter by letter a sub neural network multiple times until the appropriate word is triggered

for instance, reading "word" would trigger all words with the letter "w", then "wo", then "wor" and finally "word". As the same sub neural network gets activated multiple times, its residual activation keeps increasing as more and more letters of the word are read.

-----

-->> Try to map the problem of learning mario to a function optimization problem

How to encode in a neural network avoidance patterns? It's easy to encode in the network what we want it to do, but not what not to do.

-----

How to grow a mind

* Universal data structure framework
* Universal language for representing all these form of structure -> using graphs

-----

Unsupervised classification learning
