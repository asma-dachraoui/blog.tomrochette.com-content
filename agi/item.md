---
title: Artificial General Intelligence
date: 2015-01-01T13:30:14-05:00
taxonomy:
    type: post
    category: [artificial general intelligence]
---

# Artificial General Intelligence

[TOC]

* Computation/Processing
* Memory
	* Storage
		* Compression
	* Retrieval

-----

To limit the dangers of AI, contain them in a physical body (which can be destroyed).
By extension, this means that human should be able to extend their reach through externalization, that is, provide APIs (function library) of their thought process so that others may query that process.

-----

Concepts are basically numbers
Everything can be represented by a number (basically the concept of primary key -> set of data)
We learn to associate images, sounds, experiences (samples) with a specific concept (number)

-----

# Memory/thinking as a state machine

Memory is the construction of a finite?/nondeterministic? state machine
Thought is the process of going through the state machine
Memory can reconfigure itself based on input

- Reconfigure itself how?
- How often does it reconfigure itself?

In a sense, a state machine with probabilistic paths resembles a neural network
In order to process ideas quickly, the state machine must have a central node with a immense breadth so that it will easily trigger the appropriate path based on the current input. Once the input is processed, the state returns to this central node which awaits further input. This main process is basically consciousness.

-----

DNA is the software of life. If that is true, who wrote the code?
One may extrapolate that the bigbang is similar to the generation of random code and that everything that follows is simply random permutation/mutation of the randomness that ended up into something that is coherent. Like a well programmed neural network, with enough time, randomness will start to generate patterns.

DNA: Transfer/evolution of genetic software

We are an gigantic assembly of billions of cell-sized machines. Each and every cell contains its own copy of the program (DNA) which is itself about 3 billion nucleotides. As there are 4 valid nucleotides/bases, (2^2)^(3*10^9) possible combinations.

-----

If the big bang is true, then we are a simulation. Every thought, action, atom current property is defined in a table at time t = x.

-----

<pre>
<tomzx> well, my understanding of turing so far is that you can represent pretty much anything as a number
<tomzx> except those non-computable numbers
<tomzx> so every word can be represented as a number, phrase (order of words) as a number, documents as a number, thoughts as a number, etc.
<tomzx> basically everything can be labelled
<tomzx> then you can "easily" say A <-> B
<tomzx> in the sense that the entity represented by A is related to the entity represented by B
<tomzx> although I don't think that gets us very far
</pre>

-----

A super-intelligent AI decides that the best use of matter/energy in its vicinity is to nano-engineer the raw materials necessary for its mind to expand. It sends out quadrillions of self-replicating probes in a spherical region, programmed to reduce all matter they encounter into the "smart" matter which the AI can use as its brain. 2500 years later, give or take, there is a massive 5000 light year sphere of "empty" space. It's actually all cognitive matter.

=> is this the reduction to a Turing machine? space is basically just information encoded

-----

I believe intelligence emerges from complexity. By stacking simple concepts onto one another, intelligence appears.

-----

Reading a word is basically triggering letter by letter a sub neural network multiple times until the appropriate word is triggered

for instance, reading "word" would trigger all words with the letter "w", then "wo", then "wor" and finally "word". As the same sub neural network gets activated multiple times, its residual activation keeps increasing as more and more letters of the word are read.

-----

# Things it should do

* Improve algorithms
	* Learn which data structure is the most appropriate for a problem
		* Inspect code and be able to figure out if the current data structure is the best one for its current use
* Improve data structures

# Important properties
* Signal filtering (ignore non-essential data to reduce the domain size)
* Abstraction/simplification/class generation. Learn to group similar stimuli so that you do not have to learn about each of them individually.

-->> Try to map the problem of learning mario to a function optimization problem

How to encode in a neural network avoidance patterns? It's easy to encode in the network what we want it to do, but not what not to do.

-----

How to grow a mind

* Universal data structure framework
* Universal language for representing all these form of structure -> using graphs
