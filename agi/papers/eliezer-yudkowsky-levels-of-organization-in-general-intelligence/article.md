---
title: Eliezer Yudkowsky - Levels of Organization in General Intelligence (2007)
created: 2016-04-14
taxonomy:
  category: [Artificial General Intelligence]
  status: in progress
---

## Context

## Learned in this study

## Things to explore

# Overview

## Mistakes in AI
* Implementing a process too close to the token level - trying to implement a high-level process without implementing the underlying layers of organization
* The failure to distinguish between things that can be hardwired and things that must be learned

## 1 Foundations of General Intelligence
* What is intelligence? A brain with a hundred billion neurons and a hundred trillion synapses; a brain in which the cerebral cortex alone is organized into 52 cytoarchitecturally distinct areas per hemisphere
* Intelligence is the complex expression of a complex set of principles
* Intelligence is a supersystem composed of many mutually interdependent subsystems - subsystems specialized not only for particular environmental skills but for particular internal functions
* Intelligence requires more than passive correspondence between internal representations and sensory data, or between sensory data and reality
* Intelligence in the fully human sense requires the ability to manipulate the world by reasoning backward from a mental image of the desired outcome to create a mental image of the necessary actions
* A new (genetic) adaptation requiring the presence of a previous adaptation cannot spread unless the prerequisite adaptation is present in the genetic environment with sufficient statistical regularity to make the new adaptation a recurring evolutionary advantage
* Evolution often finds good ways, but rarely the best ways
* Evolution is a useful inspiration but a dangerous template
* Because evolution lacks foresight, complex functions cannot evolve unless their prerequisites are evolutionary advantages for other reasons
* A deliberately designed AI is likely to begin as a set of subsystems in a relatively primitive and undeveloped state, but nonetheless already designed to form a functioning supersystem
* Where the human line developed from very complex non-general intelligence into very complex general intelligence, a successful AI project is more likely to develop from a primitive general intelligence into a complex general intelligence
* The abstract knowledge that biological neurons implement human intelligence does not explain human intelligence
* The most common paradigms of traditional AI - search trees, neural networks, genetic algorithms, evolutionary computation, semantic nets - have in common the property that they can be implemented without requiring a store of preexisting complexity
* Today we have at least one reason to believe that nonsensory intelligence is a bad approach; we tried it and it didn't work
* Yudkowsky argues strongly for "supersystems," but he does not believe that "supersystems" are the necessary and sufficient Key to AI
* General intelligence requires the right supersystem, with the right cognitive subsystems, doing the right things in the right way
* Humans are not intelligent by virtue of being "supersystems," but by virtue of being a particular supersystem which implements human intelligence

## 2 Levels of Organization in Deliberative General Intelligence

## 2.1 Concepts: An Illustration of Principles
* DGI hypothesizes that visualization occurs through a flow from high-level feature controllers to low-level feature controllers, creating an articulated mental image within a sensory modality trhough a multistage process that allows the detection of conflicts at higher levels before proceeding to lower levels
* The final mental imagery is introspectively visible, but the process that creates it is mostly opaque
* While the cat has roughly $10^6$ fibers from the lateral geniculate nucleus to the visual cortex, there are approximately $10^7$ fibers running in the opposite direction
* No explanatory consensus currently exists for the existence of the massive corticothalamic feedback projects, though there are many competing theory
* In concept combination, a few flashes of the intermediate stages of processing may be visible as introspective glimpses - especially those conflicts that arise to the level of conscious attention before being resolved automatically
* Almost all of the internal complexity of concepts is hidden away from human introspection, and many theories of AI (even in the modern era) thus attempt to implement concepts on the token level, e.g., "lightbulb" as a raw LISP atom
* DGI belongs to the existing tradition that ask, not "How do we ground our semantic nets?", but rather "What is the underlying stuff making up these rich high-level objects we call 'symbols'?"
* The "frame problem" - sometimes also considered a form of the "commonsense problem" - in which intelligent reasoning appears to require knowledge of an infinite number of special cases
* Consider a CPU which adds two 32-bit numbers
* An "expert system" that encodes a vast semantic network containing the "knowledge" that 2+2=4, 21+16=37 and so on would require a lookup table of 18 billion billion entries
* Symbol grounding: Linking G0025 to hamburger or Johnny Carson
* Yudkowsky believes that many (though not all) cases of the "commonsense problem" or "frame problem" arise from trying to store all possible descriptions of high-level behaviors that, in the human mind, are modeled by visualizing the lower level of organization from which those behaviors emerge
* The visual cortex "knows" about edge detection, shading, textures of curved surfaces, binocular disparities, color constancy under natural lighting, motion relative to the plane of fixation, and so on
* Even if an AI needs programmer-created concepts to bootstrap further concept formation, bootstrap concepts should be created using programmer-directed tool versions of the corresponding AI subsystems, and the bootstrap concepts should be replaced with the AI-formed concepts as early as possible
* Two potential problems emerging from the use of programmer-created content are opacity and isolation
	* Opacity refers to the potential inability of an AI's subsystems to modify content that orginated outside the AI
		* If a programmer is creating cognitive content, it should at least be the kind of content that the AI could have created on its own; it should be content in a form that the AI's cognitive subsystems can manipulate
	* Isolation means that if a concept, or a piece of knowledge, is handed to the AI on a silver platter, the AI may be isolated from the things that the AI would have needed to learn first in order to acquire that knowledge naturally, in the course of building up successive layers of understanding to handle problems of increasing complexity


# See also

# Sources
