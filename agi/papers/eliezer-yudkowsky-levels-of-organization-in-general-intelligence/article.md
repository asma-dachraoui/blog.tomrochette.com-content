---
title: Eliezer Yudkowsky - Levels of Organization in General Intelligence (2007)
created: 2016-04-14
taxonomy:
  category: [Artificial General Intelligence]
  status: in progress
---

## Context

## Learned in this study

## Things to explore

# Overview

# 1 Foundations of General Intelligence
* What is intelligence? A brain with a hundred billion neurons and a hundred trillion synapses; a brain in which the cerebral cortex alone is organized into 52 cytoarchitecturally distinct areas per hemisphere
* Intelligence is the complex expression of a complex set of principles
* Intelligence is a supersystem composed of many mutually interdependent subsystems - subsystems specialized not only for particular environmental skills but for particular internal functions
* Intelligence requires more than passive correspondence between internal representations and sensory data, or between sensory data and reality
* Intelligence in the fully human sense requires the ability to manipulate the world by reasoning backward from a mental image of the desired outcome to create a mental image of the necessary actions
* A new (genetic) adaptation requiring the presence of a previous adaptation cannot spread unless the prerequisite adaptation is present in the genetic environment with sufficient statistical regularity to make the new adaptation a recurring evolutionary advantage
* Evolution often finds good ways, but rarely the best ways
* Evolution is a useful inspiration but a dangerous template
* Because evolution lacks foresight, complex functions cannot evolve unless their prerequisites are evolutionary advantages for other reasons
* A deliberately designed AI is likely to begin as a set of subsystems in a relatively primitive and undeveloped state, but nonetheless already designed to form a functioning supersystem
* Where the human line developed from very complex non-general intelligence into very complex general intelligence, a successful AI project is more likely to develop from a primitive general intelligence into a complex general intelligence
* The abstract knowledge that biological neurons implement human intelligence does not explain human intelligence
* The most common paradigms of traditional AI - search trees, neural networks, genetic algorithms, evolutionary computation, semantic nets - have in common the property that they can be implemented without requiring a store of preexisting complexity
* Today we have at least one reason to believe that nonsensory intelligence is a bad approach; we tried it and it didn't work
* Yudkowsky argues strongly for "supersystems," but he does not believe that "supersystems" are the necessary and sufficient Key to AI
* General intelligence requires the right supersystem, with the right cognitive subsystems, doing the right things in the right way
* Humans are not intelligent by virtue of being "supersystems," but by virtue of being a particular supersystem which implements human intelligence

# See also

# Sources
